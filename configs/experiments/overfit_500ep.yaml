# Refined overfitting config - 500 epochs
# Key changes from baseline:
# - 500 epochs (5x more training)
# - Higher LR: 0.0003 (3x for faster convergence)
# - Shorter warmup: 50 steps
# - Lower weight decay: 0.001 (less regularization)
# - Disabled coordinate augmentation (see exact same data)

output_dir: data/catgen/overfit_exp/refined_500ep

experiment:
  name: overfit_refined
  version: 1.0.0
  seed: 42
  deterministic: true

code_version: refined_500ep

data:
  train_lmdb_path: dataset/toy_small/dataset.lmdb
  val_lmdb_path: dataset/toy_small/dataset.lmdb
  test_lmdb_path: null
  batch_size:
    train: 32
    val: 32
    test: 32
  num_workers:
    train: 4
    val: 4
    test: 4
  preload_to_ram: true
  use_pyg: false
  train_shuffle: false

model:
  hidden_dim: 768
  flow_model_args:
    transformer_depth: 24
    transformer_heads: 12
    positional_encoding: false
    attention_impl: flash
    activation_checkpointing: false
  prior_sampler_args:
    coord_std: 1.0
    lognormal_loc: [1.5845935274470484, 1.8048289191348783, 2.062939587081105]
    lognormal_scale: [0.3456883565588731, 0.4261945604282258, 0.4394933811412375]
    uniform_low: 60.0
    uniform_high: 120.0
    uniform_eps: 0.1
    supercell_mean: [[-0.026726, 0.07897, -0.06539], [-0.133529, 0.188983, -0.132132], [-0.013621, -0.254989, 0.140161]]
    supercell_std: [[1.7964, 1.213596, 0.8779], [1.223385, 1.383904, 1.337392], [1.373271, 1.452034, 1.045282]]
    prim_slab_coord_mean: [3.298159, 4.591314, 5.311862]
    prim_slab_coord_std: [2.842205, 3.540208, 4.057026]
    ads_coord_mean: [0.007402, -0.702398, 0.934607]
    ads_coord_std: [8.24743, 9.035212, 9.979235]
    scaling_factor_mean: 3.42256
    scaling_factor_std: 0.473372
    prim_virtual_mean: [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]
    prim_virtual_std: [[5.0, 5.0, 5.0], [5.0, 5.0, 5.0], [5.0, 5.0, 5.0]]
    supercell_virtual_mean: [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]
    supercell_virtual_std: [[10.0, 10.0, 10.0], [10.0, 10.0, 10.0], [10.0, 10.0, 10.0]]
  flow_process_args:
    num_sampling_steps: 50
    synchronize_timesteps: false
    compile_model: false
    timestep_distribution: exponential
    use_time_reweighting: true
  use_kernels: false

training:
  optimizer: adamw
  max_epochs: 500  # 5x more epochs
  accelerator: gpu
  devices: 1
  strategy: auto
  precision: bf16-mixed
  accumulate_grad_batches: 1
  num_sanity_val_steps: 0
  gradient_clip_val: 10.0
  gradient_clip_algorithm: norm
  val_check_interval: null
  check_val_every_n_epoch: 100  # Validate every 100 epochs
  log_every_n_steps: 1

  limit_train_batches: null
  limit_val_batches: null

  train_multiplicity: 1
  loss_type: l2
  loss_space: normalized
  prim_slab_coord_loss_weight: 1.0
  ads_coord_loss_weight: 1.0
  prim_virtual_loss_weight: 1.0
  supercell_virtual_loss_weight: 1.0
  scaling_factor_loss_weight: 1.0

  lr: 0.0003  # 3x higher for faster convergence
  weight_decay: 0.001  # Lower regularization
  adam_beta1: 0.9
  adam_beta2: 0.999

  use_ema: true
  ema_decay: 0.9999

  scheduler:
    type: cosine_warmup
    warmup_steps: 50  # Shorter warmup
    warmup_epochs: null
    min_lr_ratio: 0.01

  checkpoint:
    save_top_k: 3
    save_last: true
    monitor: train/loss/total
    mode: min

validation:
  sample_every_n_epochs: 1
  flow_samples: 1
  sampling_steps: 50
  sampling_center_coords: false
  timeout: 120
  num_workers: 4
  compute_adsorption: false
  adsorption_device: cuda
  adsorption_model: uma-m-1p1
  adsorption_timeout: 100
  structure_matcher_args:
    ltol: 0.3
    stol: 0.5
    angle_tol: 10.0
  log_structures_to_wandb: true
  max_structures_to_log: 5

prediction:
  num_samples: 1
  sampling_steps: 50
  sampling_center_coords: false
  refine_final: false

wandb:
  enabled: true
  project: CatGen
  entity: null
  mode: online
  tags: [overfit_small, refined, 500ep]
  notes: "Refined overfitting: 500ep, higher LR, no coord aug"
  watch_gradients: true
  watch_freq: 100
