# Overfitting test configuration
# Purpose: Train on a few samples to verify the model can memorize them
# If the model is working correctly, training loss should approach zero

output_dir: "data/catgen/overfit_test"

experiment:
  name: "overfit_test"
  version: "1.0.0"
  seed: 42
  deterministic: true

code_version: "overfit"

# Data configuration - single sample for overfitting
data:
  train_lmdb_path: "dataset/train/dataset.lmdb"
  val_lmdb_path: "dataset/train/dataset.lmdb"  # Same as train for overfitting validation
  test_lmdb_path: null
  batch_size:
    train: 1  # Single sample for maximum overfitting
    val: 1
    test: 1
  num_workers:
    train: 0  # Single worker for deterministic ordering
    val: 0
    test: 0
  preload_to_ram: true
  use_pyg: false
  train_shuffle: false  # CRITICAL: Disable shuffle to see same sample each epoch

# Model architecture (same as default for fair comparison)
model:
  atom_s: 256
  token_s: 256

  flow_model_args:
    atom_encoder_depth: 2
    atom_encoder_heads: 4
    atom_encoder_positional_encoding: false
    token_transformer_depth: 4
    token_transformer_heads: 4
    atom_decoder_depth: 2
    atom_decoder_heads: 4
    attention_impl: "flash"
    activation_checkpointing: false
    dng: false  # Disable DNG for deterministic overfitting (no random atom count sampling)

  prior_sampler_args:
    coord_std: 1.0
    lognormal_loc: [1.5845935274470484, 1.8048289191348783, 2.0629395870811051]
    lognormal_scale: [0.3456883565588731, 0.4261945604282258, 0.4394933811412375]
    uniform_low: 60.0
    uniform_high: 120.0
    uniform_eps: 0.1
    supercell_mean: [
      [-0.026726, 0.078970, -0.065390],
      [-0.133529, 0.188983, -0.132132],
      [-0.013621, -0.254989, 0.140161]
    ]
    supercell_std: [
      [1.796400, 1.213596, 0.877900],
      [1.223385, 1.383904, 1.337392],
      [1.373271, 1.452034, 1.045282]
    ]
    prim_slab_coord_mean: [3.298159, 4.591314, 5.311862]
    prim_slab_coord_std: [2.842205, 3.540208, 4.057026]
    ads_coord_mean: [0.007402, -0.702398, 0.934607]
    ads_coord_std: [8.247430, 9.035212, 9.979235]
    scaling_factor_mean: 3.422560
    scaling_factor_std: 0.473372
    # Virtual coord normalizers (identity for now, will be computed from data)
    prim_virtual_mean: [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]
    prim_virtual_std: [[5.0, 5.0, 5.0], [5.0, 5.0, 5.0], [5.0, 5.0, 5.0]]
    supercell_virtual_mean: [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]
    supercell_virtual_std: [[10.0, 10.0, 10.0], [10.0, 10.0, 10.0], [10.0, 10.0, 10.0]]

  flow_process_args:
    num_sampling_steps: 50
    coordinate_augmentation: false  # Disable augmentation for pure overfitting
    synchronize_timesteps: false
    compile_model: false
    timestep_distribution: "uniform"  # Uniform for simpler training signal
    use_time_reweighting: false  # Disable for simpler loss
    # Note: fixed_timestep removed - model needs to learn at all t values for sampling to work
    fixed_prior_seed: 42  # Fix prior noise for deterministic training (critical for overfitting)

  use_kernels: false

# Training configuration optimized for overfitting
training:
  max_epochs: 5000  # Full overfitting test
  accelerator: "gpu"
  devices: 1
  strategy: "auto"
  precision: "bf16-mixed"
  accumulate_grad_batches: 1
  num_sanity_val_steps: 0  # Skip sanity check
  gradient_clip_val: 1.0  # Stricter gradient clipping for stability
  gradient_clip_algorithm: "norm"
  val_check_interval: 1.0
  check_val_every_n_epoch: 5000  # Only validate at end

  # Limit to 1 batch per epoch (1 sample with batch_size=1)
  limit_train_batches: 1
  limit_val_batches: 1  # Run validation on 1 batch

  # Loss configuration
  # HYPOTHESIS: Higher multiplicity provides denser timestep coverage for flow matching
  # With multiplicity=16, prim_slab coords failed (RMSD 8.5Ã…) while other outputs succeeded
  # Increasing to 128 should provide better gradient signal for high-dimensional prim_slab prediction
  train_multiplicity: 128  # Increased from 16 - critical for learning high-dimensional coordinate flows
  loss_type: "l2"
  prim_slab_coord_loss_weight: 1.0
  ads_coord_loss_weight: 1.0
  prim_virtual_loss_weight: 1.0
  supercell_virtual_loss_weight: 1.0
  scaling_factor_loss_weight: 1.0
  prim_slab_element_loss_weight: 5.0

  # Optimizer - high initial LR with cosine decay
  lr: 0.001  # Moderate LR for testing with bounded outputs
  weight_decay: 0.0  # No regularization
  adam_beta1: 0.9
  adam_beta2: 0.999

  # Disable EMA for pure training signal observation
  use_ema: false
  ema_decay: 0.9999

  # Cosine annealing scheduler for smooth convergence
  scheduler:
    type: "cosine_warmup"  # Fixed: was "cosine" which caused "Unknown scheduler type" warning
    warmup_steps: 100
    warmup_epochs: null
    min_lr_ratio: 0.01  # Decay to 1% of initial LR

  # Checkpointing
  checkpoint:
    save_top_k: 1
    save_last: true
    monitor: "train/total_loss"  # Monitor train loss for overfitting
    mode: "min"

# Validation configuration - run at end to check sample quality
validation:
  sample_every_n_epochs: 1  # Sample when validation runs (only at end)
  flow_samples: 1
  sampling_steps: 50
  sampling_center_coords: false
  sampling_seed: 42  # Fixed seed for deterministic validation sampling (critical for overfitting)
  timeout: 120
  num_workers: 4
  n_prim_slab_atoms_histogram_path: "primitive_atom_distribution.json"
  max_n_prim_slab_atoms: null
  compute_adsorption: false
  adsorption_device: "cuda"
  adsorption_model: "uma-m-1p1"
  adsorption_timeout: 100
  structure_matcher_args:
    ltol: 0.3
    stol: 0.5
    angle_tol: 10.0
  log_structures_to_wandb: false
  max_structures_to_log: 0
  # Training match rate: compare generated samples against training set to detect memorization
  compute_train_match_rate: true
  max_train_cache_size: 500  # Maximum training samples to cache for comparison

# Prediction configuration
prediction:
  num_samples: 1
  sampling_steps: 50
  sampling_center_coords: false
  refine_final: false

# WandB enabled to track overfitting
wandb:
  enabled: true
  project: "CatGen"
  entity: null
  mode: "online"
  tags: ["overfit"]
  notes: "Overfitting test to verify model training"
  watch_gradients: true
  watch_freq: 50
