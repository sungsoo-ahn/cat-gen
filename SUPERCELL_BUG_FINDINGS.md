# Supercell Matrix Bug Analysis - Findings & Recommendations

## Configuration Analysis Results

### Virtual Coordinate Normalization Settings

**Default config (`configs/default/default.yaml`):**
- `coord_output_scale`: NOT SET (uses default 3.0 from code)
- `prim_virtual_std`: 5.0 (all components)
- `supercell_virtual_std`: 10.0 (all components)

**Effective ranges after denormalization:**
- Primitive virtual: `tanh(x) * 3.0 * 5.0 = [-15.0, 15.0]` Angstroms ‚úì
- Supercell virtual: `tanh(x) * 3.0 * 10.0 = [-30.0, 30.0]` Angstroms ‚úì

**Verdict:** **ACCEPTABLE** - Should cover most lattice parameters (typically 5-15 √Ö)

---

**Overfit config (`configs/default/overfit.yaml`):**
- `coord_output_scale`: **1.0** (explicitly reduced, line 46)
- `prim_virtual_std`: 5.0
- `supercell_virtual_std`: 10.0

**Effective ranges after denormalization:**
- Primitive virtual: `tanh(x) * 1.0 * 5.0 = [-5.0, 5.0]` Angstroms ‚ö†Ô∏è
- Supercell virtual: `tanh(x) * 1.0 * 10.0 = [-10.0, 10.0]` Angstroms ‚ö†Ô∏è

**Verdict:** **PROBLEMATIC** - Too restrictive for typical lattice parameters!
- Lattice components are often 8-15 √Ö
- The model cannot represent large lattice vectors
- **This explains why overfitting may be difficult!**

---

## Critical Issues Identified

### üî¥ Issue #1: Overfit Config Has Insufficient Output Range

**Location:** `configs/default/overfit.yaml:46`

```yaml
coord_output_scale: 1.0  # Reduced from 3.0 to bound initial predictions tighter
```

**Problem:**
- Reduces output range by 3x
- Primitive lattice vectors limited to [-5, 5] √Ö per component
- Typical lattice parameters: a, b, c ‚àà [8, 15] √Ö
- **Model physically cannot predict many valid structures!**

**Impact:**
- Overfitting experiment cannot succeed
- Model predictions will saturate at boundaries
- Training loss will plateau at a high value
- Generated structures will be distorted or invalid

**Fix:**
```yaml
# Option 1: Remove the line (use default 3.0)
# coord_output_scale: 1.0

# Option 2: Increase to appropriate value
coord_output_scale: 3.0

# Option 3: Use different scales for different outputs
# (requires code change)
```

---

### üü° Issue #2: No Supercell Matrix Validation in Assembly

**Location:** `src/catgen/scripts/assemble.py:158-165`

**Problem:**
- Model predictions are rounded to integers without validation
- If `det(rounded_matrix) < 1`, pymatgen will fail or produce invalid supercells
- No error prevention, only error handling after failure

**Evidence from config:**
Lines 277-281 of assemble.py have error catching, suggesting this happens:
```python
except Exception as e:
    print(f"[WARNING] Failed to reconstruct sample {i}: {e}")
    print(f"  - Problematic Supercell Matrix:\n{generated_supercell_matrices[i]}")
```

**Recommended fix:**
```python
# After rounding (line 161):
supercell_matrix = np.round(generated_supercell_matrix).astype(int)

# Add validation:
det = int(round(np.linalg.det(supercell_matrix)))
if det < 1:
    # Use refine_sc_mat to find nearest valid matrix
    from src.catgen.scripts.refine_sc_mat import refine_sc_mat
    supercell_matrix = refine_sc_mat(
        torch.tensor(supercell_matrix, dtype=torch.float32)
    ).numpy().astype(int)
    print(f"[INFO] Refined invalid supercell matrix (det={det}) to valid matrix")
```

---

### üü° Issue #3: Numerical Instability in Matrix Inversion

**Location:** `src/catgen/data/conversions.py:65`

```python
def compute_supercell_matrix_from_vectors(prim_vectors, supercell_vectors):
    return supercell_vectors @ np.linalg.inv(prim_vectors)
```

**Problem:**
- No conditioning check
- No error handling for singular/near-singular matrices
- Could produce NaN/Inf if model predicts degenerate lattice

**Recommended fix:**
```python
def compute_supercell_matrix_from_vectors(prim_vectors, supercell_vectors):
    """Recover supercell matrix from primitive and supercell lattice vectors.

    Uses pseudo-inverse for numerical stability.
    """
    # Check conditioning
    cond = np.linalg.cond(prim_vectors)
    if cond > 1e10:
        import warnings
        warnings.warn(f"Ill-conditioned prim_vectors (cond={cond:.2e}), using pseudo-inverse")
        prim_inv = np.linalg.pinv(prim_vectors, rcond=1e-8)
    else:
        prim_inv = np.linalg.inv(prim_vectors)

    return supercell_vectors @ prim_inv
```

---

## Mathematical Correctness: ‚úÖ VERIFIED

After code review and theoretical analysis:

‚úÖ **Forward transformation is correct:**
```python
supercell_virtual_coords = torch.bmm(supercell_matrix, prim_virtual_coords)
# Computes: supercell_vectors = S @ prim_vectors (rows = vectors)
```

‚úÖ **Backward transformation is correct:**
```python
supercell_matrix = supercell_vectors @ inv(prim_vectors)
# Recovers: S from SC @ inv(P)
```

‚úÖ **Pymatgen compatibility verified:**
- Both use rows as lattice vectors
- Both use `S @ P` transformation
- No transpose needed

‚úÖ **Determinant preservation:**
- det(SC) = det(S) √ó det(P)
- Verified algebraically

---

## Action Items

### Immediate (Critical)

1. **Fix overfit config:**
   ```bash
   # Edit configs/default/overfit.yaml line 46
   # Change: coord_output_scale: 1.0
   # To:     coord_output_scale: 3.0
   ```

2. **Verify current training:**
   ```bash
   # Check if current runs use overfit config
   ls data/catgen/overfit/logs/ 2>/dev/null
   # If exists, training may be stuck due to insufficient output range
   ```

### Short-term (Important)

3. **Add supercell matrix validation:**
   - Enable refinement in assemble.py
   - Uncomment or add validation code

4. **Improve numerical stability:**
   - Update conversion functions to use pseudo-inverse
   - Add condition number checks

### Long-term (Enhancement)

5. **Separate output scales:**
   - Different `output_scale` for coords vs virtual atoms
   - Possibly remove tanh for virtual atoms

6. **Better supercell prior:**
   - Sample near integer matrices
   - Use discrete diffusion

---

## Testing Recommendations

### Test 1: Verify Output Range

```python
# Check if model can represent typical structures
import torch
from src.catgen.data.prior import CatPriorSampler

# Create sampler with overfit config settings
sampler = CatPriorSampler(
    coord_std=1.0,
    prim_virtual_std=[[5.0]*3]*3,
    supercell_virtual_std=[[10.0]*3]*3,
)

# Test: Can we represent a 12 √Ö lattice vector?
test_vector = torch.tensor([[[12.0, 0.0, 0.0], [0.0, 12.0, 0.0], [0.0, 0.0, 12.0]]])
normalized = sampler.normalize_prim_virtual(test_vector)

# With coord_output_scale=1.0:
# normalized = 12/5 = 2.4
# tanh(2.4) ‚âà 0.98
# output = 0.98 * 1.0 = 0.98
# denormalized = 0.98 * 5 = 4.9 √Ö  ‚Üê WRONG! Should be 12 √Ö

# With coord_output_scale=3.0:
# output = 0.98 * 3.0 = 2.94
# denormalized = 2.94 * 5 = 14.7 √Ö  ‚Üê CLOSE! (small error acceptable)
```

### Test 2: Check Supercell Matrix Validity

```bash
# Look for reconstruction failures in logs
find data/ -name "*.log" -exec grep -l "Failed to reconstruct" {} \;

# Count failures
grep "Failed to reconstruct" data/*/logs/*.log 2>/dev/null | wc -l
```

### Test 3: Monitor for NaN/Inf

```bash
# Check training logs for numerical issues
grep -i "nan\|inf" data/*/logs/*.log 2>/dev/null
```

---

## Summary

| Issue | Severity | Status | Fix Required |
|-------|----------|--------|--------------|
| Overfit config output range | üî¥ Critical | **FOUND** | Change `coord_output_scale: 3.0` |
| Supercell matrix validation | üü° Major | Known | Add validation code |
| Matrix inversion stability | üü° Major | Potential | Use pseudo-inverse |
| dtype precision loss | üü† Moderate | Acceptable | Consider improving |
| Prior distribution | üü† Moderate | Design choice | No immediate action |
| Mathematical correctness | ‚úÖ Verified | **CORRECT** | No issues found |

---

## Confidence Levels

- üî¥ **Overfit config bug:** 100% confidence - clearly visible in config
- üü° **Validation missing:** 95% confidence - error handling code confirms
- üü° **Numerical stability:** 80% confidence - no reported issues yet
- ‚úÖ **Math correctness:** 99% confidence - verified algebraically

---

## Next Steps

**Recommended immediate action:**

```bash
cd /home/user/cat-gen

# 1. Fix the overfit config
# Edit configs/default/overfit.yaml line 46
# Change coord_output_scale from 1.0 to 3.0

# 2. Test if current training is affected
ls -lah data/catgen/overfit/checkpoints/ 2>/dev/null

# 3. If training exists, check if it plateaued
# Look at loss curves in WandB or tensorboard

# 4. Consider restarting training with fixed config
```
